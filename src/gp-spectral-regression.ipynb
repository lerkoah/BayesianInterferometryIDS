{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "# Training data is 11 points in [0,1] inclusive regularly spaced\n",
    "train_x = Variable(torch.linspace(0, 1, 11))\n",
    "# True function is sin(2*pi*x) with Gaussian noise N(0,0.04)\n",
    "train_y = Variable(torch.sin(train_x.data * (2 * np.pi)) + torch.randn(train_x.size()) * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood\n",
    "from gpytorch.random_variables import GaussianRandomVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of RBFKernel()>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, W):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        # Our mean function is constant in the interval [-1,1]\n",
    "        self.mean_module = ConstantMean(constant_bounds=(-1, 1))\n",
    "        # We use the RBF kernel as a universal approximator\n",
    "        self.covar_module = RBFKernel(log_lengthscale_bounds=(-10, 10))\n",
    "        \n",
    "        self.W = torch.from_numpy(W).float()\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "#         print(covar_x.type())\n",
    "#         print(self.W.type())\n",
    "        covar_x = self.W.t().matmul(covar_x.matmul(self.W))\n",
    "        # Return moddl output as GaussianRandomVariable\n",
    "        return GaussianRandomVariable(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = GaussianLikelihood(log_noise_bounds=(-5, 5))\n",
    "# W = 1*np.ones((train_x.data.shape[0], train_x.data.shape[0]), dtype=np.float)\n",
    "W = np.random.randn(train_x.data.shape[0], train_x.data.shape[0])\n",
    "W = np.matmul(W.T,W)\n",
    "\n",
    "model = ExactGPModel(train_x.data, train_y.data, likelihood, W)\n",
    "model.covar_module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lerko/anaconda3/lib/python3.6/site-packages/gpytorch/functions/add_diag.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  val = diag.squeeze()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/100 - Loss: 1.579   log_lengthscale: 0.000   log_noise: 0.000\n",
      "Iter 2/100 - Loss: 1.581   log_lengthscale: -0.100   log_noise: -0.100\n",
      "Iter 3/100 - Loss: 1.569   log_lengthscale: -0.200   log_noise: -0.200\n",
      "Iter 4/100 - Loss: 1.583   log_lengthscale: -0.183   log_noise: -0.300\n",
      "Iter 5/100 - Loss: 1.582   log_lengthscale: -0.156   log_noise: -0.399\n",
      "Iter 6/100 - Loss: 1.598   log_lengthscale: -0.107   log_noise: -0.498\n",
      "Iter 7/100 - Loss: 1.450   log_lengthscale: -0.077   log_noise: -0.596\n",
      "Iter 8/100 - Loss: 1.548   log_lengthscale: -0.068   log_noise: -0.694\n",
      "Iter 9/100 - Loss: 1.605   log_lengthscale: -0.046   log_noise: -0.792\n",
      "Iter 10/100 - Loss: 1.658   log_lengthscale: -0.050   log_noise: -0.887\n",
      "Iter 11/100 - Loss: 1.685   log_lengthscale: -0.049   log_noise: -0.980\n",
      "Iter 12/100 - Loss: 1.408   log_lengthscale: -0.074   log_noise: -1.071\n",
      "Iter 13/100 - Loss: 1.639   log_lengthscale: -0.096   log_noise: -1.161\n",
      "Iter 14/100 - Loss: 1.639   log_lengthscale: -0.111   log_noise: -1.248\n",
      "Iter 15/100 - Loss: 1.539   log_lengthscale: -0.121   log_noise: -1.331\n",
      "Iter 16/100 - Loss: 1.524   log_lengthscale: -0.121   log_noise: -1.409\n",
      "Iter 17/100 - Loss: 1.366   log_lengthscale: -0.118   log_noise: -1.485\n",
      "Iter 18/100 - Loss: 1.381   log_lengthscale: -0.117   log_noise: -1.559\n",
      "Iter 19/100 - Loss: 1.296   log_lengthscale: -0.134   log_noise: -1.629\n",
      "Iter 20/100 - Loss: 1.269   log_lengthscale: -0.140   log_noise: -1.696\n",
      "Iter 21/100 - Loss: 1.330   log_lengthscale: -0.145   log_noise: -1.758\n",
      "Iter 22/100 - Loss: 1.027   log_lengthscale: -0.129   log_noise: -1.816\n",
      "Iter 23/100 - Loss: 1.390   log_lengthscale: -0.130   log_noise: -1.870\n",
      "Iter 24/100 - Loss: 1.579   log_lengthscale: -0.139   log_noise: -1.918\n",
      "Iter 25/100 - Loss: 1.312   log_lengthscale: -0.152   log_noise: -1.956\n",
      "Iter 26/100 - Loss: 1.356   log_lengthscale: -0.148   log_noise: -1.987\n",
      "Iter 27/100 - Loss: 1.254   log_lengthscale: -0.126   log_noise: -2.011\n",
      "Iter 28/100 - Loss: 1.471   log_lengthscale: -0.103   log_noise: -2.027\n",
      "Iter 29/100 - Loss: 1.227   log_lengthscale: -0.087   log_noise: -2.036\n",
      "Iter 30/100 - Loss: 1.314   log_lengthscale: -0.066   log_noise: -2.039\n",
      "Iter 31/100 - Loss: 1.158   log_lengthscale: -0.042   log_noise: -2.036\n",
      "Iter 32/100 - Loss: 1.597   log_lengthscale: -0.002   log_noise: -2.028\n",
      "Iter 33/100 - Loss: 1.334   log_lengthscale: 0.030   log_noise: -2.011\n",
      "Iter 34/100 - Loss: 1.708   log_lengthscale: 0.051   log_noise: -1.991\n",
      "Iter 35/100 - Loss: 1.306   log_lengthscale: 0.068   log_noise: -1.963\n",
      "Iter 36/100 - Loss: 1.879   log_lengthscale: 0.091   log_noise: -1.932\n",
      "Iter 37/100 - Loss: 1.427   log_lengthscale: 0.110   log_noise: -1.895\n",
      "Iter 38/100 - Loss: 0.983   log_lengthscale: 0.139   log_noise: -1.856\n",
      "Iter 39/100 - Loss: 1.662   log_lengthscale: 0.169   log_noise: -1.819\n",
      "Iter 40/100 - Loss: 1.486   log_lengthscale: 0.185   log_noise: -1.780\n",
      "Iter 41/100 - Loss: 1.493   log_lengthscale: 0.218   log_noise: -1.740\n",
      "Iter 42/100 - Loss: 1.475   log_lengthscale: 0.228   log_noise: -1.703\n",
      "Iter 43/100 - Loss: 1.883   log_lengthscale: 0.216   log_noise: -1.666\n",
      "Iter 44/100 - Loss: 1.606   log_lengthscale: 0.196   log_noise: -1.627\n",
      "Iter 45/100 - Loss: 1.369   log_lengthscale: 0.186   log_noise: -1.589\n",
      "Iter 46/100 - Loss: 1.152   log_lengthscale: 0.170   log_noise: -1.557\n",
      "Iter 47/100 - Loss: 1.218   log_lengthscale: 0.152   log_noise: -1.531\n",
      "Iter 48/100 - Loss: 1.440   log_lengthscale: 0.128   log_noise: -1.513\n",
      "Iter 49/100 - Loss: 1.205   log_lengthscale: 0.102   log_noise: -1.497\n",
      "Iter 50/100 - Loss: 1.362   log_lengthscale: 0.072   log_noise: -1.488\n",
      "Iter 51/100 - Loss: 1.176   log_lengthscale: 0.063   log_noise: -1.483\n",
      "Iter 52/100 - Loss: 1.521   log_lengthscale: 0.047   log_noise: -1.486\n",
      "Iter 53/100 - Loss: 1.345   log_lengthscale: 0.052   log_noise: -1.491\n",
      "Iter 54/100 - Loss: 1.412   log_lengthscale: 0.054   log_noise: -1.500\n",
      "Iter 55/100 - Loss: 1.695   log_lengthscale: 0.043   log_noise: -1.512\n",
      "Iter 56/100 - Loss: 1.277   log_lengthscale: 0.031   log_noise: -1.524\n",
      "Iter 57/100 - Loss: 1.827   log_lengthscale: 0.006   log_noise: -1.540\n",
      "Iter 58/100 - Loss: 1.128   log_lengthscale: -0.020   log_noise: -1.554\n",
      "Iter 59/100 - Loss: 1.292   log_lengthscale: -0.049   log_noise: -1.575\n",
      "Iter 60/100 - Loss: 1.571   log_lengthscale: -0.070   log_noise: -1.599\n",
      "Iter 61/100 - Loss: 1.151   log_lengthscale: -0.094   log_noise: -1.621\n",
      "Iter 62/100 - Loss: 1.567   log_lengthscale: -0.107   log_noise: -1.647\n",
      "Iter 63/100 - Loss: 1.444   log_lengthscale: -0.111   log_noise: -1.672\n",
      "Iter 64/100 - Loss: 1.012   log_lengthscale: -0.118   log_noise: -1.696\n",
      "Iter 65/100 - Loss: 1.572   log_lengthscale: -0.113   log_noise: -1.725\n",
      "Iter 66/100 - Loss: 1.420   log_lengthscale: -0.108   log_noise: -1.751\n",
      "Iter 67/100 - Loss: 1.490   log_lengthscale: -0.119   log_noise: -1.773\n",
      "Iter 68/100 - Loss: 1.532   log_lengthscale: -0.143   log_noise: -1.794\n",
      "Iter 69/100 - Loss: 1.298   log_lengthscale: -0.163   log_noise: -1.810\n",
      "Iter 70/100 - Loss: 1.280   log_lengthscale: -0.181   log_noise: -1.825\n",
      "Iter 71/100 - Loss: 0.986   log_lengthscale: -0.200   log_noise: -1.840\n",
      "Iter 72/100 - Loss: 1.314   log_lengthscale: -0.205   log_noise: -1.855\n",
      "Iter 73/100 - Loss: 1.430   log_lengthscale: -0.215   log_noise: -1.869\n",
      "Iter 74/100 - Loss: 1.394   log_lengthscale: -0.193   log_noise: -1.877\n",
      "Iter 75/100 - Loss: 1.281   log_lengthscale: -0.167   log_noise: -1.881\n",
      "Iter 76/100 - Loss: 1.429   log_lengthscale: -0.145   log_noise: -1.884\n",
      "Iter 77/100 - Loss: 0.892   log_lengthscale: -0.136   log_noise: -1.883\n",
      "Iter 78/100 - Loss: 1.425   log_lengthscale: -0.130   log_noise: -1.885\n",
      "Iter 79/100 - Loss: 1.516   log_lengthscale: -0.113   log_noise: -1.883\n",
      "Iter 80/100 - Loss: 1.873   log_lengthscale: -0.089   log_noise: -1.875\n",
      "Iter 81/100 - Loss: 1.181   log_lengthscale: -0.074   log_noise: -1.861\n",
      "Iter 82/100 - Loss: 1.250   log_lengthscale: -0.050   log_noise: -1.846\n",
      "Iter 83/100 - Loss: 1.376   log_lengthscale: -0.023   log_noise: -1.831\n",
      "Iter 84/100 - Loss: 1.247   log_lengthscale: -0.001   log_noise: -1.815\n",
      "Iter 85/100 - Loss: 1.824   log_lengthscale: 0.016   log_noise: -1.799\n",
      "Iter 86/100 - Loss: 1.322   log_lengthscale: -0.002   log_noise: -1.777\n",
      "Iter 87/100 - Loss: 1.570   log_lengthscale: -0.016   log_noise: -1.756\n",
      "Iter 88/100 - Loss: 1.596   log_lengthscale: -0.028   log_noise: -1.734\n",
      "Iter 89/100 - Loss: 1.615   log_lengthscale: -0.041   log_noise: -1.712\n",
      "Iter 90/100 - Loss: 1.541   log_lengthscale: -0.066   log_noise: -1.691\n",
      "Iter 91/100 - Loss: 1.264   log_lengthscale: -0.092   log_noise: -1.669\n",
      "Iter 92/100 - Loss: 1.873   log_lengthscale: -0.109   log_noise: -1.653\n",
      "Iter 93/100 - Loss: 1.597   log_lengthscale: -0.132   log_noise: -1.636\n",
      "Iter 94/100 - Loss: 1.744   log_lengthscale: -0.164   log_noise: -1.622\n",
      "Iter 95/100 - Loss: 1.282   log_lengthscale: -0.181   log_noise: -1.610\n",
      "Iter 96/100 - Loss: 1.342   log_lengthscale: -0.187   log_noise: -1.607\n",
      "Iter 97/100 - Loss: 1.318   log_lengthscale: -0.187   log_noise: -1.612\n",
      "Iter 98/100 - Loss: 1.139   log_lengthscale: -0.185   log_noise: -1.621\n",
      "Iter 99/100 - Loss: 1.234   log_lengthscale: -0.193   log_noise: -1.639\n",
      "Iter 100/100 - Loss: 1.201   log_lengthscale: -0.201   log_noise: -1.658\n"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 100\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   log_lengthscale: %.3f   log_noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.data[0],\n",
    "        model.covar_module.log_lengthscale.data[0, 0],\n",
    "        model.likelihood.log_noise.data[0]\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8179124315538594"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-0.201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [62 x 62], m2: [11 x 11] at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/TH/generic/THTensorMath.c:2033",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8d38821a1001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Make predictions by feeding model through likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_pred_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mobserved_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Define plotting function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lerko/anaconda3/lib/python3.6/site-packages/gpytorch/models/exact_gp.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Exact inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mfull_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mfull_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExactGP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfull_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGaussianRandomVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ExactGP.forward must return a GaussianRandomVariable'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/lerko/anaconda3/lib/python3.6/site-packages/gpytorch/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 raise RuntimeError('Input must be a RandomVariable or Variable, was a %s' %\n\u001b[1;32m    164\u001b[0m                                    input.__class__.__name__)\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomVariable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyVariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-e3dc9fb2bc82>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#         print(covar_x.type())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#         print(self.W.type())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mcovar_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcovar_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Return moddl output as GaussianRandomVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mGaussianRandomVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovar_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [62 x 62], m2: [11 x 11] at /opt/conda/conda-bld/pytorch_1524584710464/work/aten/src/TH/generic/THTensorMath.c:2033"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQwAAADGCAYAAAAniL71AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc0f42914e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put model and likelihood into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize plot\n",
    "f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "# Test points are regularly spaced along [0,1] every 0.02\n",
    "test_x = Variable(torch.linspace(0, 1, 51))\n",
    "# Make predictions by feeding model through likelihood\n",
    "with gpytorch.fast_pred_var():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "# Define plotting function\n",
    "def ax_plot(ax, rand_var, title):\n",
    "    # Get upper and lower confidence bounds\n",
    "    lower, upper = rand_var.confidence_region()\n",
    "    # Plot training data as black stars\n",
    "    ax.plot(train_x.data.numpy(), train_y.data.numpy(), 'k*')\n",
    "    # Plot predictive means as blue line\n",
    "    ax.plot(test_x.data.numpy(), rand_var.mean().data.numpy(), 'b')\n",
    "    # Shade between the lower and upper confidence bounds\n",
    "    ax.fill_between(test_x.data.numpy(), lower.data.numpy(), upper.data.numpy(), alpha=0.5)\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title(title)\n",
    "# Plot the predictions\n",
    "ax_plot(observed_ax, observed_pred, 'Observed Values (Likelihood)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.cuda.FloatTensor with no dimension]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
